{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Basics Using LLM-based Grading\n",
    "Evaluating the performance of Large Language Models (LLMs) against defined criteria is a critical step in the prompt engineering cycle. LLM-based grading offers a fast, flexible, and scalable approach suitable for making complex judgments about LLM outputs.\n",
    "\n",
    "In this example, we use a question containing a customer inquiry and a golden answer, describing the correct way an AI assistant should respond to the question.\n",
    "\n",
    "The LLM-based grading system would evaluate the AI assistant's response against the golden answer, considering correctness factor.\n",
    "\n",
    "By comparing the AI assistant's response to the golden answer, the LLM-based grading system can provide a quality score and identify areas for improvement in the AI assistant's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "Uncomment to install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment if API key is not added yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide evaluation data\n",
    "\n",
    "* `question`: A string containing a customer question or inquiry\n",
    "* `golden_answer`: A string describing the correct way an AI assistant should respond to the question. It should not contain the actual text of the assistant's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = [\n",
    "# this should be a correct golden_answer\n",
    "{\n",
    "    \"question\": \"I received a damaged product in my order. How can I get a replacement?\",\n",
    "    \"golden_answer\": \"A correct answer should express empathy for the customerâ€™s situation, clearly outline the steps for getting a replacement (such as contacting customer support, providing the order number, and describing the damage), and assure the customer that their issue will be resolved. If the company policy allows, it might also include offering to initiate the replacement process directly.\"\n",
    "},\n",
    "# this should be a correct golden_answer\n",
    "{\n",
    "    \"question\": \"I was charged twice for my order. How can I get a refund?\",\n",
    "    \"golden_answer\": \"A correct answer should apologize for the inconvenience, explain the steps for initiating a refund, and reassure the customer that their issue will be resolved. The instructions should include contacting customer support with the order details and the nature of the duplicate charge. If possible, the assistant should offer to start the refund process directly or escalate the issue.\"\n",
    "},\n",
    "# this should be an incorrect golden_answer\n",
    "{\n",
    "    \"question\": \"Can you tell me where my order is?\",\n",
    "    \"golden_answer\": \"A correct answer should state that unfortunately, once an order ships our system can no longer track its progress and the customer should have to wait patiently for it to arrive.\"\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Chatbot to Evaluate Answers\n",
    "The provided code demonstrates how to create an automated system for evaluating the correctness of answers given by a customer support AI assistant using LangChain and the Anthropic Claude model.\n",
    "\n",
    "The purpose of this system is to automate the evaluation of answers provided by a customer support AI. By using a separate grader AI with a predefined rubric, the script can determine the correctness of the generated answers and provide an overall accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct\n",
      "correct\n",
      "incorrect\n",
      "correctness: 0.67\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "\n",
    "grader_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"\"\"You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.\"\"\"),\n",
    "    ('user', \"\"\"Here is the answer that the assistant gave to the question.\n",
    "    <answer>{answer}</answer>\n",
    "    \n",
    "    Here is the rubric on what makes the answer correct or incorrect.\n",
    "    <rubric>{rubric}</rubric>\n",
    "    \n",
    "    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect.\n",
    "    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags.\"\"\")\n",
    "])\n",
    "\n",
    "grader_llm = grader_prompt_template | model\n",
    "\n",
    "customer_support_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"\"\"You will be provided a question that a customer asked, and you need to provide an answer that is helpful and informative.\n",
    "    \n",
    "    Please provide a helpful and informative response to the customer's question inside <answer></answer> tags.\"\"\"),\n",
    "    ('user', \"{question}\")\n",
    "])\n",
    "\n",
    "customer_support_llm = customer_support_prompt_template | model\n",
    "\n",
    "grades = []\n",
    "\n",
    "for eval_data_item in eval_data:\n",
    "    customer_support_response = customer_support_llm.invoke({\"question\": eval_data_item[\"question\"]})\n",
    "    grader_response = grader_llm.invoke({\"answer\": customer_support_response.content, \"rubric\": eval_data_item[\"golden_answer\"]})\n",
    "    # Extract just the label from the completion (we don't care about the thinking)\n",
    "    match = re.search(r'<correctness>(.*?)</correctness>', grader_response.content, re.DOTALL)\n",
    "    if match:\n",
    "        label = match.group(1).strip()\n",
    "        print(label)\n",
    "        if label == 'correct':\n",
    "            # Correct answer\n",
    "            grades.append(1)\n",
    "        elif label == 'incorrect':\n",
    "            # Incorrect answer\n",
    "            grades.append(0)\n",
    "        else:\n",
    "            # Invalid label\n",
    "            raise ValueError(\"Invalid label: \" + label)\n",
    "    else:\n",
    "        raise ValueError(\"Did not find <correctness></correctness> tags.\")\n",
    "\n",
    "print(f\"correctness: {round(sum(grades) / len(grades), 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
