{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# LangChain RAG Using Local Embeddings for PDF\n",
    "Implementing a RAG system using the LangChain framework, with a focus on:\n",
    "* Generating local vector embeddings for efficient similarity search\n",
    "* Indexing and querying a PDF document to find relevant passages\n",
    "* Utilizing the retrieved passages to generate answers to questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "Uncomment to install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# pip install -U langchain-anthropic langchain_community langchain_chroma pypdf sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment if API key is not added yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The Document\n",
    "The document was downloaded from https://investor.fb.com/financials/ and saved as a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "FILE_PATH = \"example_data/meta-10k-2023.pdf\"\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSITION REPOR T PURSUANT  TO SECTION 13 OR 15(d) OF  THE SECURITIES EXCHANGE ACT  OF 1934\n",
      "For the transition period fr om            to            \n",
      "Commission File Number: 001-35551\n",
      "__________________________\n",
      "Meta Platforms, Inc.\n",
      "(Exact name of r egistrant as specified in its charter)\n",
      "__________________________\n",
      "Delawar e 20-1665019\n",
      "(State or other \n",
      "{'source': 'example_data/meta-10k-2023.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[300:650])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Embeddings\n",
    "This section describes how to generate embeddings locally without relying on third-party services like OpenAI. We use the `all-MiniLM-L6-v2` model, which has the following characteristics:\n",
    "\n",
    "1. Model size: Approximately 40 MB\n",
    "2. Storage: Model downloaded automatically once and stored locally\n",
    "3. Functionality: Generates embeddings for text data\n",
    "\n",
    "By using local embeddings, you maintain control over your data processing pipeline and reduce dependencies on external services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return [self.model.encode(d).tolist() for d in documents]\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.model.encode([query])[0].tolist()\n",
    "    \n",
    "embedding_model = CustomEmbeddings(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Documents for Retrieval\n",
    "1. Use a text splitter to divide loaded documents into smaller chunks. This ensures each segment fits within the LLM's context window.\n",
    "\n",
    "2. Load the split documents into a vector store. This process typically involves converting text into numerical representations (vectors) for efficient searching.\n",
    "\n",
    "3. Implement a retriever based on the vector store. This component will be responsible for fetching relevant document segments during the question-answering process.\n",
    "\n",
    "4. Incorporate the retriever into your Retrieval-Augmented Generation (RAG) pipeline. This enables the LLM to access and utilize relevant information from the processed documents when generating responses.\n",
    "\n",
    "Note:\n",
    "* Chroma is useful here because it provides an efficient, scalable, and semantically-aware way to store and retrieve vectorized text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering with RAG\n",
    "To construct the final RAG chain, you'll utilize built-in helper functions. The process yields two key results:\n",
    "\n",
    "1. Final Answer: Available in the 'answer' key of the results dictionary.\n",
    "2. Context: The information the Language Model (LLM) used to generate the answer.\n",
    "\n",
    "Examining the 'context' values reveals:\n",
    "- Documents containing chunks of the ingested page content\n",
    "- Preserved original metadata from the initial document loading phase\n",
    "\n",
    "This structure allows you to trace the answer's origin and understand the LLM's reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"What was Meta's revenue in 2023?\", 'context': [Document(metadata={'page': 75, 'source': 'example_data/meta-10k-2023.pdf'}, page_content='Reality Labs\\nRL revenue in 2023 decreased $263 million, or 12%, compared to 2022. The decrease in RL revenue was mostly driven by a net decrease in the volume\\nof Meta Quest sales.\\nRevenue Seasonality\\nRevenue is traditionally seasonally strong in the fourth quarter of each year due in part to seasonal holiday demand. We believe that this seasonality in\\nboth advertising revenue and RL consumer hardware sales affects our quarterly results, which generally reflect significant growth in revenue between the third\\nand fourth quarters and a decline between the fourth and subsequent first quarters. For instance, our total revenue increased 17%, 16%, and 16% between the\\nthird and fourth quarters of 2023, 2022, and 2021, respectively, while total revenue for the first quarters of 2023, 2022, and 2021 declined 11%, 17%, and 7%\\ncompared to the fourth quarters of 2022, 2021, and 2020 respectively.\\n75'), Document(metadata={'page': 7, 'source': 'example_data/meta-10k-2023.pdf'}, page_content='infrastructure as part of our efforts to develop our apps and our advertising services. We are also making significant investments in our metaverse efforts,\\nincluding developing virtual and augmented reality devices, software for social platforms, neural interfaces, and other foundational technologies. Our total RL\\ninvestments were $18.02 billion in 2023 and include expenses relating to headcount and technology development across these efforts. These are fundamentally\\nnew technologies that we expect will evolve as the metaverse ecosystem develops, and many products for the metaverse may only be fully realized in the next\\ndecade. Although it is inherently difficult to predict when and how the metaverse ecosystem will develop, we expect our RL segment to continue to operate at a\\nloss for the foreseeable future, and our ability to support our metaverse efforts is dependent on generating sufficient profits from other areas of our business. We'), Document(metadata={'page': 75, 'source': 'example_data/meta-10k-2023.pdf'}, page_content='and measurement tools have had a favorable impact on our ad performance and advertising demand. Other factors are also discussed in the section entitled \"â€”\\nExecutive Overview of Full Year 2023 Results.\" In addition, year-over-year advertising revenue growth for the full year 2023 was driven mainly by marketer\\nspending in online commerce, which benefited from marketers based in China, consumer packaged goods, and entertainment and media. We anticipate that\\nfuture advertising revenue will be driven by a combination of price and the number of ads delivered.\\nOther revenue\\nFoA other revenue in 2023 increased $250 million, or 31%, compared to 2022. The increase was mainly driven by WhatsApp Business Platform\\nrevenue.\\nReality Labs\\nRL revenue in 2023 decreased $263 million, or 12%, compared to 2022. The decrease in RL revenue was mostly driven by a net decrease in the volume\\nof Meta Quest sales.\\nRevenue Seasonality'), Document(metadata={'page': 75, 'source': 'example_data/meta-10k-2023.pdf'}, page_content='Table of Contents\\nRevenue\\nThe following table sets forth our revenue by source and by segment:\\n Year Ended December 31,\\n 2023 2022 20212023 vs 2022 %\\nchange2022 vs 2021 %\\nchange\\n(in millions, except percentages)\\nAdvertising $ 131,948 $ 113,642 $ 114,934 16 % (1)%\\nOther revenue 1,058 808 721 31 % 12 %\\nFamily of Apps 133,006 114,450 115,655 16 % (1)%\\nReality Labs 1,896 2,159 2,274 (12)% (5)%\\nTotal revenue $ 134,902 $ 116,609 $ 117,929 16 % (1)%\\nFamily of Apps\\nFoA revenue in 2023 increased $18.56 billion, or 16%, compared to 2022. The increase was almost entirely driven by advertising revenue.\\nAdvertising\\nAdvertising revenue in 2023 increased $18.31 billion, or 16%, compared to 2022 due to an increase in the number of ads delivered, partially offset by a\\ndecrease in the average price per ad. In 2023, the number of ads delivered increased by 28%, as compared with an 18% increase in 2022 as ads impressions')], 'answer': \"According to the context provided, Meta's total revenue in 2023 was $134.9 billion. This consisted of $131.9 billion from advertising revenue and $1.9 billion from Reality Labs (virtual and augmented reality) revenue.\"}\n",
      "According to the context provided, Meta's total revenue in 2023 was $134.9 billion. This consisted of $131.9 billion from advertising revenue and $1.9 billion from Reality Labs (virtual and augmented reality) revenue.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"<context>\"\n",
    "    \"{context}\"\n",
    "    \"</context>\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What was Meta's revenue in 2023?\"})\n",
    "\n",
    "print(results)\n",
    "print(results[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
