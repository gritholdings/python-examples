{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# LangChain RAG Using Local Embeddings for PDF\n",
    "Implementing a RAG system using the LangChain framework, with a focus on:\n",
    "* Generating local vector embeddings for efficient similarity search\n",
    "* Indexing and querying a PDF document to find relevant passages\n",
    "* Utilizing the retrieved passages to generate answers to questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "Uncomment to install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# pip install -U langchain-anthropic langchain_community langchain_chroma pypdf sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment if API key is not added yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Embeddings\n",
    "This section describes how to generate embeddings locally without relying on third-party services like OpenAI. We use the `all-MiniLM-L6-v2` model, which has the following characteristics:\n",
    "\n",
    "1. Model size: Approximately 40 MB\n",
    "2. Storage: Model downloaded automatically once and stored locally\n",
    "3. Functionality: Generates embeddings for text data\n",
    "\n",
    "By using local embeddings, you maintain control over your data processing pipeline and reduce dependencies on external services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific tokenizer warning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, \n",
    "                       message=\"`clean_up_tokenization_spaces` was not set\")\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return [self.model.encode(d).tolist() for d in documents]\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.model.encode([query])[0].tolist()\n",
    "    \n",
    "embedding_model = CustomEmbeddings(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple QA system\n",
    "\n",
    "### `add_pdfs`: Loading The Document\n",
    "The document was downloaded from the websites. Then, they are saved as local files.\n",
    "\n",
    "To prepare documents for Retrieval\n",
    "1. Use a text splitter to divide loaded documents into smaller chunks. This ensures each segment fits within the LLM's context window.\n",
    "2. Load the split documents into a vector store. This process typically involves converting text into numerical representations (vectors) for efficient searching.\n",
    "3. Implement a retriever based on the vector store. This component will be responsible for fetching relevant document segments during the question-answering process.\n",
    "4. Incorporate the retriever into your Retrieval-Augmented Generation (RAG) pipeline. This enables the LLM to access and utilize relevant information from the processed documents when generating responses.\n",
    "\n",
    "### `_setup_qa_chain`: Question Answering with RAG\n",
    "To construct the final RAG chain, you'll utilize built-in helper functions. The process yields two key results:\n",
    "\n",
    "1. Final Answer: Available in the 'answer' key of the results dictionary.\n",
    "2. Context: The information the Language Model (LLM) used to generate the answer.\n",
    "\n",
    "Examining the 'context' values reveals:\n",
    "- Documents containing chunks of the ingested page content\n",
    "- Preserved original metadata from the initial document loading phase\n",
    "\n",
    "This structure allows you to trace the answer's origin and understand the LLM's reasoning process.\n",
    "\n",
    "Note:\n",
    "* Chroma is useful here because it provides an efficient, scalable, and semantically-aware way to store and retrieve vectorized text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class PDFQuestionAnswering:\n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        # Initialize embedding model\n",
    "        self.embedding_model = CustomEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        \n",
    "        # Initialize empty vectorstore\n",
    "        self.vectorstore = Chroma(\n",
    "            embedding_function=self.embedding_model\n",
    "        )\n",
    "        \n",
    "        # Setup QA chain\n",
    "        self.qa_chain = self._setup_qa_chain()\n",
    "    \n",
    "    def add_pdfs(self, pdf_files: Union[str, List[str]]) -> None:\n",
    "        \"\"\"Add one or more PDFs to the existing system.\"\"\"\n",
    "        # Handle single file input\n",
    "        if isinstance(pdf_files, str):\n",
    "            pdf_files = [pdf_files]\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        # Process each PDF file\n",
    "        for pdf_path in pdf_files:\n",
    "            try:\n",
    "                # Validate file\n",
    "                path = Path(pdf_path)\n",
    "                if not path.exists():\n",
    "                    print(f\"File not found: {pdf_path}\")\n",
    "                    continue\n",
    "                if path.suffix.lower() != '.pdf':\n",
    "                    print(f\"Not a PDF file: {pdf_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Load and process document\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                docs = loader.load()\n",
    "                splits = self.text_splitter.split_documents(docs)\n",
    "                documents.extend(splits)\n",
    "                print(f\"Successfully loaded: {pdf_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "        \n",
    "        # Add to vectorstore if documents were loaded\n",
    "        if documents:\n",
    "            self.vectorstore.add_documents(documents)\n",
    "            print(f\"Added {len(documents)} chunks to the vectorstore\")\n",
    "    \n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"Ask a question to the system.\"\"\"\n",
    "        response = self.qa_chain.invoke({\"question\": question})\n",
    "        return response\n",
    "    \n",
    "    def _setup_qa_chain(self):\n",
    "        \"\"\"Setup the QA chain with Claude.\"\"\"\n",
    "        llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\")\n",
    "        \n",
    "        # Create the prompt template\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", (\"You are an assistant for question-answering tasks. \"\n",
    "                \"Use the following pieces of retrieved context to answer \"\n",
    "                \"the question. If you don't know the answer, say that you \"\n",
    "                \"don't know. Use three sentences maximum and keep the \"\n",
    "                \"answer concise.\"\n",
    "                \"\\n\\n\"\n",
    "                \"<context>\"\n",
    "                \"{context}\"\n",
    "                \"</context>\")),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        \n",
    "        # Setup retrieval chain using the new LCEL interface\n",
    "        retriever = self.vectorstore.as_retriever()\n",
    "        \n",
    "        chain = (\n",
    "            {\"context\": retriever, \"question\": lambda x: x}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: example_data/meta-10k-2023.pdf\n",
      "Added 147 chunks to the vectorstore\n",
      "According to the context provided, Meta's Reality Labs (RL) revenue in 2023 decreased $263 million, or 12%, compared to 2022. The decrease was primarily driven by a net decrease in the volume of Meta Quest (virtual reality headset) sales.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the system\n",
    "qa_system = PDFQuestionAnswering(chunk_size=10000, chunk_overlap=2000)\n",
    "\n",
    "# Add initial PDFs\n",
    "# Source: https://investor.fb.com/financials/\n",
    "qa_system.add_pdfs([\"example_data/meta-10k-2023.pdf\"])\n",
    "\n",
    "# Ask questions\n",
    "answer = qa_system.ask(\"How's Meta's Reality Labs revenue in 2023?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Later, add more PDFs\n",
    "As you ask questions, you can add more PDFs to add more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: example_data/amazon-10k-2023.pdf\n",
      "Added 123 chunks to the vectorstore\n",
      "The primary revenue streams for Meta (Facebook) are advertising revenue from its Family of Apps (Facebook, Instagram, WhatsApp, etc.) and other minor revenue sources. For Amazon, the key revenue streams are retail product sales, third-party seller fees (commission and fulfillment), Amazon Web Services (AWS), advertising services, and subscriptions like Amazon Prime.\n",
      "\n",
      "Over recent years, Meta's advertising revenue has remained dominant but saw a slight decline in 2022 before rebounding in 2023. Amazon's retail sales and AWS have continued growing, with AWS becoming an increasingly important revenue driver alongside the retail business.\n"
     ]
    }
   ],
   "source": [
    "# https://www.sec.gov/Archives/edgar/data/1018724/000101872424000008/amzn-20231231.htm\n",
    "qa_system.add_pdfs([\"example_data/amazon-10k-2023.pdf\"])\n",
    "\n",
    "# Ask questions about old and new content\n",
    "answer = qa_system.ask(\"What are the primary revenue streams for Meta and Amazon, and how have they evolved over recent fiscal years?\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
